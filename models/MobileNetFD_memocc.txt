Generating project at location ./Test_MobileNetFD/
Weights updated up to layer 47
Max IM2COL size of 221184 bytes @layer 0
Additional 0 bytes allocated for mixed precision management (size @layer 0, Input)
Layer 0 (conv2d):  Input: 98304, Coefficients: 1728, Biases: 0, Output: 262144, Total: 362176 (data + gradients + biases)
Layer 1 (ReLU):  Input: 262144, Coefficients: 0, Biases: 0, Output: 262144, Total: 524288 (data + gradients + biases)
Layer 2 (DW):  Input: 262144, Coefficients: 576, Biases: 0, Output: 69696, Total: 332416 (data + gradients + biases)
Layer 3 (ReLU):  Input: 65536, Coefficients: 0, Biases: 0, Output: 65536, Total: 131072 (data + gradients + biases)
Layer 4 (PW):  Input: 65536, Coefficients: 4096, Biases: 0, Output: 131072, Total: 200704 (data + gradients + biases)
Layer 5 (ReLU):  Input: 131072, Coefficients: 0, Biases: 0, Output: 131072, Total: 262144 (data + gradients + biases)
Layer 6 (DW):  Input: 131072, Coefficients: 1152, Biases: 0, Output: 36992, Total: 169216 (data + gradients + biases)
Layer 7 (ReLU):  Input: 32768, Coefficients: 0, Biases: 0, Output: 32768, Total: 65536 (data + gradients + biases)
Layer 8 (PW):  Input: 32768, Coefficients: 16384, Biases: 0, Output: 65536, Total: 114688 (data + gradients + biases)
Layer 9 (ReLU):  Input: 65536, Coefficients: 0, Biases: 0, Output: 65536, Total: 131072 (data + gradients + biases)
Layer 10 (DW):  Input: 65536, Coefficients: 2304, Biases: 0, Output: 65536, Total: 133376 (data + gradients + biases)
Layer 11 (ReLU):  Input: 65536, Coefficients: 0, Biases: 0, Output: 65536, Total: 131072 (data + gradients + biases)
Layer 12 (PW):  Input: 65536, Coefficients: 32768, Biases: 0, Output: 65536, Total: 163840 (data + gradients + biases)
Layer 13 (ReLU):  Input: 65536, Coefficients: 0, Biases: 0, Output: 65536, Total: 131072 (data + gradients + biases)
Layer 14 (DW):  Input: 65536, Coefficients: 2304, Biases: 0, Output: 20736, Total: 88576 (data + gradients + biases)
Layer 15 (ReLU):  Input: 16384, Coefficients: 0, Biases: 0, Output: 16384, Total: 32768 (data + gradients + biases)
Layer 16 (PW):  Input: 16384, Coefficients: 65536, Biases: 0, Output: 32768, Total: 114688 (data + gradients + biases)
Layer 17 (ReLU):  Input: 32768, Coefficients: 0, Biases: 0, Output: 32768, Total: 65536 (data + gradients + biases)
Layer 18 (DW):  Input: 32768, Coefficients: 4608, Biases: 0, Output: 32768, Total: 70144 (data + gradients + biases)
Layer 19 (ReLU):  Input: 32768, Coefficients: 0, Biases: 0, Output: 32768, Total: 65536 (data + gradients + biases)
Layer 20 (PW):  Input: 32768, Coefficients: 131072, Biases: 0, Output: 32768, Total: 196608 (data + gradients + biases)
Layer 21 (ReLU):  Input: 32768, Coefficients: 0, Biases: 0, Output: 32768, Total: 65536 (data + gradients + biases)
Layer 22 (DW):  Input: 32768, Coefficients: 4608, Biases: 0, Output: 12800, Total: 50176 (data + gradients + biases)
Layer 23 (ReLU):  Input: 8192, Coefficients: 0, Biases: 0, Output: 8192, Total: 16384 (data + gradients + biases)
Layer 24 (PW):  Input: 8192, Coefficients: 262144, Biases: 0, Output: 16384, Total: 286720 (data + gradients + biases)
Layer 25 (ReLU):  Input: 16384, Coefficients: 0, Biases: 0, Output: 16384, Total: 32768 (data + gradients + biases)
Layer 26 (DW):  Input: 16384, Coefficients: 9216, Biases: 0, Output: 16384, Total: 41984 (data + gradients + biases)
Layer 27 (ReLU):  Input: 16384, Coefficients: 0, Biases: 0, Output: 16384, Total: 32768 (data + gradients + biases)
Layer 28 (PW):  Input: 16384, Coefficients: 524288, Biases: 0, Output: 16384, Total: 557056 (data + gradients + biases)
Layer 29 (ReLU):  Input: 16384, Coefficients: 0, Biases: 0, Output: 16384, Total: 32768 (data + gradients + biases)
Layer 30 (DW):  Input: 16384, Coefficients: 9216, Biases: 0, Output: 16384, Total: 41984 (data + gradients + biases)
Layer 31 (ReLU):  Input: 16384, Coefficients: 0, Biases: 0, Output: 16384, Total: 32768 (data + gradients + biases)
Layer 32 (PW):  Input: 16384, Coefficients: 524288, Biases: 0, Output: 16384, Total: 557056 (data + gradients + biases)
Layer 33 (ReLU):  Input: 16384, Coefficients: 0, Biases: 0, Output: 16384, Total: 32768 (data + gradients + biases)
Layer 34 (DW):  Input: 16384, Coefficients: 9216, Biases: 0, Output: 16384, Total: 41984 (data + gradients + biases)
Layer 35 (ReLU):  Input: 16384, Coefficients: 0, Biases: 0, Output: 16384, Total: 32768 (data + gradients + biases)
Layer 36 (PW):  Input: 16384, Coefficients: 524288, Biases: 0, Output: 16384, Total: 557056 (data + gradients + biases)
Layer 37 (ReLU):  Input: 16384, Coefficients: 0, Biases: 0, Output: 16384, Total: 32768 (data + gradients + biases)
Layer 38 (DW):  Input: 16384, Coefficients: 9216, Biases: 0, Output: 16384, Total: 41984 (data + gradients + biases)
Layer 39 (ReLU):  Input: 16384, Coefficients: 0, Biases: 0, Output: 16384, Total: 32768 (data + gradients + biases)
Layer 40 (PW):  Input: 16384, Coefficients: 524288, Biases: 0, Output: 16384, Total: 557056 (data + gradients + biases)
Layer 41 (ReLU):  Input: 16384, Coefficients: 0, Biases: 0, Output: 16384, Total: 32768 (data + gradients + biases)
Layer 42 (DW):  Input: 16384, Coefficients: 9216, Biases: 0, Output: 16384, Total: 41984 (data + gradients + biases)
Layer 43 (ReLU):  Input: 16384, Coefficients: 0, Biases: 0, Output: 16384, Total: 32768 (data + gradients + biases)
Layer 44 (PW):  Input: 16384, Coefficients: 1048576, Biases: 0, Output: 32768, Total: 1097728 (data + gradients + biases)
Layer 45 (ReLU):  Input: 32768, Coefficients: 0, Biases: 0, Output: 32768, Total: 65536 (data + gradients + biases)
Layer 46 (AvgPool):  Input: 32768, Coefficients: 0, Biases: 0, Output: 2048, Total: 34816 (data + gradients + biases)
Layer 47 (linear):  Input: 2048, Coefficients: 4096, Biases: 0, Output: 4, Total: 6148 (data + gradients + biases)
Max Layer size (including data and gradients): 1097728 bytes   @layer 44
Size of structures in L1 (Single Buffer Mode): 414 bytes
Size of allocated memory for labels (Single Buffer Mode): 4.0 bytes
[DNN_Size_Checker]: DNN overflows PULP L1 memory!!
Expected occupation: 1319330.0 bytes vs 131072 available L1 (1006.5689086914062%)!
Total L2 memory occupation: 2115592 bytes
DNN memory occupation: 1319330.0 bytes of 131072 available L1 bytes (1006.5689086914062%).
---------- DNN ARCHITECTURE ----------
Layer 0: FP16 conv2d, in=[3, 128, 128], wgt=[32, 3, 3, 3], out=[32, 64, 64]
Layer 1: FP16 ReLU, in=[32, 64, 64], wgt=[32, 1, 1, 32], out=[32, 64, 64]
Layer 2: FP16 DW, in=[32, 64, 64], wgt=[32, 3, 3, 32], out=[32, 33, 33]
Layer 3: FP16 ReLU, in=[32, 32, 32], wgt=[32, 1, 1, 32], out=[32, 32, 32]
Layer 4: FP16 PW, in=[32, 32, 32], wgt=[64, 1, 1, 32], out=[64, 32, 32]
Layer 5: FP16 ReLU, in=[64, 32, 32], wgt=[64, 1, 1, 64], out=[64, 32, 32]
Layer 6: FP16 DW, in=[64, 32, 32], wgt=[64, 3, 3, 64], out=[64, 17, 17]
Layer 7: FP16 ReLU, in=[64, 16, 16], wgt=[64, 1, 1, 64], out=[64, 16, 16]
Layer 8: FP16 PW, in=[64, 16, 16], wgt=[128, 1, 1, 64], out=[128, 16, 16]
Layer 9: FP16 ReLU, in=[128, 16, 16], wgt=[128, 1, 1, 128], out=[128, 16, 16]
Layer 10: FP16 DW, in=[128, 16, 16], wgt=[128, 3, 3, 128], out=[128, 16, 16]
Layer 11: FP16 ReLU, in=[128, 16, 16], wgt=[128, 1, 1, 128], out=[128, 16, 16]
Layer 12: FP16 PW, in=[128, 16, 16], wgt=[128, 1, 1, 128], out=[128, 16, 16]
Layer 13: FP16 ReLU, in=[128, 16, 16], wgt=[128, 1, 1, 128], out=[128, 16, 16]
Layer 14: FP16 DW, in=[128, 16, 16], wgt=[128, 3, 3, 128], out=[128, 9, 9]
Layer 15: FP16 ReLU, in=[128, 8, 8], wgt=[128, 1, 1, 128], out=[128, 8, 8]
Layer 16: FP16 PW, in=[128, 8, 8], wgt=[256, 1, 1, 128], out=[256, 8, 8]
Layer 17: FP16 ReLU, in=[256, 8, 8], wgt=[256, 1, 1, 256], out=[256, 8, 8]
Layer 18: FP16 DW, in=[256, 8, 8], wgt=[256, 3, 3, 256], out=[256, 8, 8]
Layer 19: FP16 ReLU, in=[256, 8, 8], wgt=[256, 1, 1, 256], out=[256, 8, 8]
Layer 20: FP16 PW, in=[256, 8, 8], wgt=[256, 1, 1, 256], out=[256, 8, 8]
Layer 21: FP16 ReLU, in=[256, 8, 8], wgt=[256, 1, 1, 256], out=[256, 8, 8]
Layer 22: FP16 DW, in=[256, 8, 8], wgt=[256, 3, 3, 256], out=[256, 5, 5]
Layer 23: FP16 ReLU, in=[256, 4, 4], wgt=[256, 1, 1, 256], out=[256, 4, 4]
Layer 24: FP16 PW, in=[256, 4, 4], wgt=[512, 1, 1, 256], out=[512, 4, 4]
Layer 25: FP16 ReLU, in=[512, 4, 4], wgt=[512, 1, 1, 512], out=[512, 4, 4]
Layer 26: FP16 DW, in=[512, 4, 4], wgt=[512, 3, 3, 512], out=[512, 4, 4]
Layer 27: FP16 ReLU, in=[512, 4, 4], wgt=[512, 1, 1, 512], out=[512, 4, 4]
Layer 28: FP16 PW, in=[512, 4, 4], wgt=[512, 1, 1, 512], out=[512, 4, 4]
Layer 29: FP16 ReLU, in=[512, 4, 4], wgt=[512, 1, 1, 512], out=[512, 4, 4]
Layer 30: FP16 DW, in=[512, 4, 4], wgt=[512, 3, 3, 512], out=[512, 4, 4]
Layer 31: FP16 ReLU, in=[512, 4, 4], wgt=[512, 1, 1, 512], out=[512, 4, 4]
Layer 32: FP16 PW, in=[512, 4, 4], wgt=[512, 1, 1, 512], out=[512, 4, 4]
Layer 33: FP16 ReLU, in=[512, 4, 4], wgt=[512, 1, 1, 512], out=[512, 4, 4]
Layer 34: FP16 DW, in=[512, 4, 4], wgt=[512, 3, 3, 512], out=[512, 4, 4]
Layer 35: FP16 ReLU, in=[512, 4, 4], wgt=[512, 1, 1, 512], out=[512, 4, 4]
Layer 36: FP16 PW, in=[512, 4, 4], wgt=[512, 1, 1, 512], out=[512, 4, 4]
Layer 37: FP16 ReLU, in=[512, 4, 4], wgt=[512, 1, 1, 512], out=[512, 4, 4]
Layer 38: FP16 DW, in=[512, 4, 4], wgt=[512, 3, 3, 512], out=[512, 4, 4]
Layer 39: FP16 ReLU, in=[512, 4, 4], wgt=[512, 1, 1, 512], out=[512, 4, 4]
Layer 40: FP16 PW, in=[512, 4, 4], wgt=[512, 1, 1, 512], out=[512, 4, 4]
Layer 41: FP16 ReLU, in=[512, 4, 4], wgt=[512, 1, 1, 512], out=[512, 4, 4]
Layer 42: FP16 DW, in=[512, 4, 4], wgt=[512, 3, 3, 512], out=[512, 4, 4]
Layer 43: FP16 ReLU, in=[512, 4, 4], wgt=[512, 1, 1, 512], out=[512, 4, 4]
Layer 44: FP16 PW, in=[512, 4, 4], wgt=[1024, 1, 1, 512], out=[1024, 4, 4]
Layer 45: FP16 ReLU, in=[1024, 4, 4], wgt=[1024, 1, 1, 1024], out=[1024, 4, 4]
Layer 46: FP16 AvgPool, in=[1024, 4, 4], wgt=[1024, 4, 4, 1024], out=[1024, 1, 1]
Layer 47: FP16 linear, in=[1024, 1, 1], wgt=[2, 1, 1, 1024], out=[2, 1, 1]
--------------------------------------
[deployment_utils.GenerateNet]: Setting last layer's output to float for PyTorch compatibility with loss function backward (future fix).
[TEST!!! max_input_dim] RES = 131072.0
[TEST!!! max_wgt_dim] RES = 524288.0
[TEST!!! max_bias_dim] RES = 0.0
No blockstranspose buffer detected.

Layer 47: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 46: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 45: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 44: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 43: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 42: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 41: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 40: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 39: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 38: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 37: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 36: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 35: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 34: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 33: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 32: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 31: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 30: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 29: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 28: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 27: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 26: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 25: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 24: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 23: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 22: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 21: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 20: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 19: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 18: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 17: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 16: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 15: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 14: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 13: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 12: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 11: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 10: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 9: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 8: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 7: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 6: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 5: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 4: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 3: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 2: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 1: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
Layer 0: stop_backprop = True, update_layer = 0 (have last_updated_layer = 47)
PULP project generation successful!
